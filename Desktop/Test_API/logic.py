"""
logic.py - Moteur de prÃ©diction des dÃ©penses (API-ready)

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                      TRANSFORMATIONS MAJEURES                              â•‘
â•‘              De autoPrediction.py (Scripts) Ã  logic.py (API)               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Š CHANGEMENT 1 : FICHIER â†’ BYTES EN MÃ‰MOIRE
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  AVANT (autoPrediction.py) :
    file_path = "C:/Users/.../depensesEtat.csv"  # Chemin sur le disque
    df = pd.read_csv(file_path)                  # Lecture du disque
  
  APRÃˆS (logic.py) :
    file_content = <bytes binaires du fichier>   # ReÃ§u de l'API (upload)
    df = pd.read_csv(io.BytesIO(file_content))   # Lecture depuis la RAM âœ¨
  
  POURQUOI ?
    â€¢ Dans une API web, les fichiers arrivent sous forme de bytes (flux binaire)
    â€¢ Lire depuis la RAM est 10x plus rapide que depuis le disque
    â€¢ SÃ©curitÃ© : pas d'Ã©criture sur le disque du serveur
    â€¢ ScalabilitÃ© : plusieurs utilisateurs simultanÃ©ment sans conflit de fichiers

ğŸ”§ CHANGEMENT 2 : input() â†’ PARAMÃˆTRES FUNCTION
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  AVANT (autoPrediction.py) :
    while True:
        months = input("Combien de mois ? ")  # Bloque l'exÃ©cution
        predict(months)
  
  APRÃˆS (logic.py) :
    def predict_from_file_content(file_content, months=12):
        # months est un PARAMÃˆTRE, pas une question interactive
  
  POURQUOI ?
    â€¢ Une API ne peut pas avoir de console interactive
    â€¢ Les paramÃ¨tres viennent de la requÃªte HTTP (GET/POST)
    â€¢ C'est non-bloquant : le serveur peut traiter d'autres demandes

ğŸ“‹ CHANGEMENT 3 : print() â†’ LOGS EN LISTE
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  AVANT (autoPrediction.py) :
    print("SaisonnalitÃ© dÃ©tectÃ©e")      # Affichage console uniquement
    print("Choix : SARIMA")             # Personne ne voit (serveur headless)
  
  APRÃˆS (logic.py) :
    self.logs.append("SaisonnalitÃ© dÃ©tectÃ©e")  # StockÃ© dans une liste
    self.logs.append("Choix : SARIMA")         # RetournÃ© Ã  l'utilisateur
  
  POURQUOI ?
    â€¢ Le serveur API n'a pas d'Ã©cran pour afficher des messages
    â€¢ L'utilisateur final doit COMPRENDRE pourquoi tel modÃ¨le a Ã©tÃ© choisi
    â€¢ Ces logs vont dans la rÃ©ponse JSON pour transparence âœ¨

ğŸ“Š CHANGEMENT 4 : plt.show() â†’ DICTIONNAIRE JSON
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  AVANT (autoPrediction.py) :
    plt.show()  # Tente d'ouvrir une fenÃªtre graphique
                # IMPOSSIBLE sur un serveur headless (sans Ã©cran) âŒ
  
  APRÃˆS (logic.py) :
    return {
        "forecast": {
            "dates": [...],      # Timestamps de prÃ©vision
            "values": [...],     # Valeurs prÃ©dites
            "upper": [...],      # Intervalle confiance sup.
            "lower": [...]       # Intervalle confiance inf.
        }
    }  # Retourne des DONNÃ‰ES (pas un graphique)
  

"""

import warnings
import pandas as pd
import numpy as np
import io                          # â† CHANGEMENT 1 : Pour lire bytes depuis RAM
from loguru import logger          # â† NOUVEAU : Logging professionnel
import os
from dotenv import load_dotenv
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tools.sm_exceptions import ConvergenceWarning
from statsmodels.tsa.holtwinters import ExponentialSmoothing
from sklearn.preprocessing import MinMaxScaler
from datetime import datetime      # â† Pour les timestamps des rÃ©ponses

warnings.filterwarnings("ignore")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONFIGURATION LOGURU (Logging Professionnel)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
load_dotenv()

# CrÃ©er le rÃ©pertoire logs s'il n'existe pas
os.makedirs("logs", exist_ok=True)

# Configuration Loguru
logger.remove()  # Supprimer handler par dÃ©faut
logger.add(
    "logs/app.log",
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>",
    level=os.getenv("LOG_LEVEL", "INFO"),
    rotation="500 MB",  # Rotation si fichier > 500 MB
    retention="7 days"  # Garder logs 7 jours
)

app_logger = logger  # Alias pour clartÃ©


# CLASSE 1 :
class DataCleaner:
    """
    CHANGEMENT MAJEUR vs autoPrediction.py :
      âŒ AVANT : file_path = "C:/Users/.../file.csv"  â†’  pd.read_csv(file_path)
      âœ“ APRÃˆS : file_content = <bytes>  â†’  pd.read_csv(io.BytesIO(file_content))

    """
    MAX_FILE_SIZE = 50 * 1024 * 1024  # â† SÃ‰CURITÃ‰ : Max 50 MB par fichier
    
    def __init__(self, file_content):
        
        self.file_content = file_content
        self.df = None
        self.logs = []  # â† CHANGEMENT : On collecte les logs au lieu de les afficher

    def _log(self, msg):
        """
        â† CHANGEMENT 3 : Au lieu de print(), on enregistre dans une liste.
        Cela permet Ã  l'API de retourner les explications Ã  l'utilisateur.
        Args:
            msg (str): Message Ã  enregistrer
        """
        self.logs.append(msg)

    def _detect_separator(self):
        try:
            first_line = self.file_content.split(b'\n')[0].decode('utf-8', errors='ignore')
            return ';' if ';' in first_line else ','
        except Exception:
            return ','

    def _find_column(self, cols, keywords):
        """
        LOGIQUE :
          Parcourt cols et cherche le premier qui contient un keyword
        """
        for c in cols:
            for key in keywords:
                if key in c:
                    return c
        return None

    def run(self):
        """
        Lance le pipeline COMPLET de nettoyage.
        
        Ã‰TAPES :
        1ï¸âƒ£  DÃ©tecte sÃ©parateur (';' ou ',')
        2ï¸âƒ£  Lit le CSV depuis les BYTES vers mÃ©moire (io.BytesIO)
        3ï¸âƒ£  Normalise les noms de colonnes (lowercase, trim)
        4ï¸âƒ£  DÃ©tecte automatiquement colonnes date et montant
        5ï¸âƒ£  Convertit montants (virgule dÃ©cimale â†’ point)
        6ï¸âƒ£  Parse les dates (intelligemment : dayfirst, etc.)
        7ï¸âƒ£  Filtre les valeurs valides (Ã©limine NaN)
        8ï¸âƒ£  AgrÃ¨ge en sÃ©rie MENSUELLE (important pour SARIMA/ARIMA)
        9ï¸âƒ£  EnlÃ¨ve dernier mois s'il est incomplet
        
        Returns:
            pd.DataFrame: Index = dates (mensuel), Colonne 'montant' = valeurs
        
        EXEMPLE OUTPUT :
            Index : 2020-01-01, 2020-02-01, 2020-03-01, ...
            Colonne 'montant' : 1000.00, 1500.50, 1200.75, ...
        """
        self._log(f"Chargement du fichier CSV...")
        
        try:
            # Validation : fichier vide / taille maximale
            if not self.file_content or len(self.file_content) == 0:
                raise ValueError("Fichier vide ou contenu invalide")
            if len(self.file_content) > self.MAX_FILE_SIZE:
                raise ValueError("trop volumineux")

            # Ã‰TAPE 1 : DÃ©tection sÃ©parateur
            sep = self._detect_separator()
            
            # â† CHANGEMENT 1 : io.BytesIO = simule un fichier depuis les bytes
            # Sans io.BytesIO, pandas ne peut pas lire les bytes directement
            self.df = pd.read_csv(io.BytesIO(self.file_content), sep=sep, encoding='utf-8', low_memory=False)
            
            # Normaliser les noms de colonnes
            self.df.columns = self.df.columns.str.strip().str.lower()

            # Ã‰TAPE 4 : DÃ©tection automatique des colonnes
            # Accepter plusieurs variantes (fr/en) courantes pour "date"
            col_date = self._find_column(self.df.columns, ['date', 'jour', 'mois', 'month', 'time', 'reglement', 'payment'])
            col_amount = self._find_column(self.df.columns, ['montant', 'sum', 'prix', 'amount', 'valeur'])

            if not col_date or not col_amount:
                raise ValueError(f"Impossible de trouver colonnes Date/Montant. Colonnes disponibles : {list(self.df.columns)}")

            self._log(f"Colonnes dÃ©tectÃ©es : date='{col_date}', montant='{col_amount}'")

            # Ã‰TAPE 5 : Nettoyage montants
            # Conversion : "1 000,50" â†’ "1000.50" â†’ 1000.50 (float)
            self.df['clean_amount'] = pd.to_numeric(
                self.df[col_amount].astype(str).str.replace('\u00A0', '').str.replace(' ', '').str.replace(',', '.'),
                errors='coerce'  # Erreur = NaN (sera supprimÃ©e aprÃ¨s)
            )
            
            # Ã‰TAPE 6 : Parsing dates
            # Prioriser dayfirst=False car de nombreux CSV utilisent le format ISO (YYYY-MM-DD)
            # Silence spÃ©cifique des UserWarning de pandas "Could not infer format..." pour Ã©viter de polluer les tests
            with warnings.catch_warnings():
                # Ignorer plusieurs messages UserWarning provenant de pandas sur l'infÃ©rence de format
                warnings.filterwarnings("ignore", message="Could not infer format.*", category=UserWarning)
                warnings.filterwarnings("ignore", message="Parsing dates in .* when dayfirst=False.*", category=UserWarning)
                self.df['clean_date'] = pd.to_datetime(self.df[col_date], dayfirst=False, errors='coerce')
                if self.df['clean_date'].isna().sum() > 0:
                    # Si dayfirst=False Ã©choue (ex: format franÃ§ais dd/mm/YYYY), essayer dayfirst=True
                    self.df['clean_date'] = pd.to_datetime(self.df[col_date], dayfirst=True, errors='coerce')
            
            # Ã‰TAPE 7 : Filtrage et indexation
            self.df = self.df.dropna(subset=['clean_date', 'clean_amount']).set_index('clean_date').sort_index()
            
            # Ã‰TAPE 8 : AgrÃ©gation en sÃ©rie mensuelle
            # Raison : SARIMA/ARIMA demandent une frÃ©quence rÃ©guliÃ¨re (ex: chaque mois)
            # Sinon les rÃ©sidus ne sont pas homogÃ¨nes
            daily = self.df['clean_amount'].resample('D').sum()  # JournaliÃ¨re d'abord
            self.df_clean = daily.resample('MS').sum().to_frame(name='montant')  # MS = 1er du mois
            
            # Ã‰TAPE 9 : Enlever dernier mois si incomplet
            # Ex : si les donnÃ©es s'arrÃªtent avant la fin du dernier mois, on l'enlÃ¨ve
            if len(self.df_clean) > 1:
                last_month_start = self.df_clean.index[-1]
                last_month_end = last_month_start + pd.offsets.MonthEnd(0)
                last_original_date = self.df.index.max()
                if last_original_date < last_month_end:
                    self.df_clean = self.df_clean.iloc[:-1]

            # SÃ‰CURITÃ‰ : VÃ©rifier qu'il reste au moins une ligne
            if self.df_clean.empty:
                self._log("ERREUR : Pas de dates valides aprÃ¨s parsing.")
                raise ValueError("Aucune date valide trouvÃ©e aprÃ¨s parsing")

            self._log(f"DonnÃ©es prÃªtes : {len(self.df_clean)} mois (de {self.df_clean.index[0].strftime('%Y-%m-%d')} Ã  {self.df_clean.index[-1].strftime('%Y-%m-%d')})")
            return self.df_clean
            
        except Exception as e:
            self._log(f"ERREUR lors du nettoyage : {str(e)}")
            raise


# ==============================================================================
# CLASSE 2 : LE PRÃ‰DICTEUR INTELLIGENT (SmartPredictor) - VERSION API
# ==============================================================================
class SmartPredictor:
    """
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â”‚ SmartPredictor - Analyse les donnÃ©es et sÃ©lectionne le meilleur modÃ¨le â”‚
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    RÃ”LE :
      1. Analyse la sÃ©rie temporelle (saisonnalitÃ©, stationnaritÃ©)
      2. SÃ©lectionne le modÃ¨le optimal parmi : AR, MA, ARMA, ARIMA, SARIMA
      3. EntraÃ®ne le modÃ¨le
      4. GÃ©nÃ¨re des prÃ©visions avec intervalles de confiance
      5. Retourne tout en dictionnaire JSON (pas de graphiques)
    
    MÃ‰THODES DE SÃ‰LECTION :
      â€¢ SaisonnalitÃ© > 10% ? â†’ SARIMA
      â€¢ Pas saisonnalitÃ© + Non-stationnaire ? â†’ ARIMA
      â€¢ Pas saisonnalitÃ© + Stationnaire ? â†’ Tournoi AR/MA/ARMA (AIC)
    
    CHANGEMENTS vs autoPrediction.py :
      âŒ AVANT : print("SaisonnalitÃ© dÃ©tectÃ©e")  â†’  Affichage console
      âœ“ APRÃˆS : self.logs.append("SaisonnalitÃ© dÃ©tectÃ©e")  â†’  StockÃ© pour API
      
      âŒ AVANT : plt.show()  â†’  Tente d'ouvrir une fenÃªtre graphique
      âœ“ APRÃˆS : return {...}  â†’  Retourne les donnÃ©es (brutes) en JSON
    """
    
    def __init__(self, df_data):
        """
        Constructeur : initialise le prÃ©dicteur avec des donnÃ©es propres.
        
        Args:
            df_data (pd.DataFrame): DataFrame avec :
                - Index : dates (mensuel)
                - Colonne 'montant' : valeurs Ã  prÃ©dire
        """
        self.df = df_data
        self.model_name = "Inconnu"
        self.order = (0, 0, 0)
        self.seasonal_order = (0, 0, 0, 0)
        self.logs = []  # â† CHANGEMENT : On collecte les explications

    def _log(self, msg):
        """
        â† CHANGEMENT 3 : Au lieu de print(), on enregistre pour l'API.
        
        Ces logs seront retournÃ©s Ã  l'utilisateur pour qu'il comprenne
        pourquoi SARIMA a Ã©tÃ© choisi plutÃ´t qu'ARIMA, par exemple.
        
        Args:
            msg (str): Message Ã  enregistrer
        """
        self.logs.append(msg)

    def _calculer_aic(self, order, seasonal_order=(0, 0, 0, 0)):
        """
        Teste un modÃ¨le SARIMAX et retourne son critÃ¨re AIC.
        
        AIC (Akaike Information Criterion) :
          â€¢ Mesure la qualitÃ© d'ajustement d'un modÃ¨le
          â€¢ PÃ©nalise les modÃ¨les trop complexes
          â€¢ PLUS BAS = MEILLEUR (c'est un critÃ¨re Ã  minimiser, comme l'erreur)
        
        UTILITÃ‰ :
          â€¢ Comparer AR vs MA vs ARMA sans expertise humaine
          â€¢ Choix objectif et reproductible
        
        Args:
            order (tuple): (p, d, q) pour la partie ARIMA
                - p : AR (AutoRegression) = mÃ©moire du passÃ©
                - d : I (Integration) = diffÃ©renciation pour stationnaritÃ©
                - q : MA (Moving Average) = lissage des erreurs
            
            seasonal_order (tuple): (P, D, Q, s) pour la saisonnalitÃ©
                - P, D, Q : comme p, d, q mais pour les saisons
                - s : pÃ©riode (12 pour donnÃ©es mensuelles = 1 an)
        
        Returns:
            float: Valeur AIC (ou inf si erreur)
        
        Exemples :
          â€¢ AR(1) : order=(1,0,0) â†’ AIC=150.5
          â€¢ MA(1) : order=(0,0,1) â†’ AIC=148.2  â† Meilleur (plus bas)
          â€¢ ARMA(1,1) : order=(1,0,1) â†’ AIC=149.8
        """
        try:
            # CrÃ©er et entraÃ®ner le modÃ¨le
            model = SARIMAX(
                self.df['montant'],
                order=order,
                seasonal_order=seasonal_order,
                enforce_stationarity=False,  # Permet de tester mÃªme si non-stationnaire
                enforce_invertibility=False  # Permet de tester mÃªme si non-inversible
            )
            results = model.fit(disp=False)  # disp=False = pas d'affichage
            return results.aic
        except Exception as e:
            self._log(f"Erreur lors du calcul AIC pour order={order} : {str(e)}")
            return float('inf')  # Si erreur, ce modÃ¨le est pÃ©nalisÃ© (AIC=âˆ)

    def _fit_holtwinters(self, seasonal_periods=12):
        """
        EntraÃ®ne un modÃ¨le Holt-Winters (ExponentialSmoothing) et retourne
        une mÃ©trique de sÃ©lection (ici AIC si disponible, sinon MSE).
        """
        try:
            model = ExponentialSmoothing(
                self.df['montant'],
                seasonal='add',
                trend='add',
                seasonal_periods=seasonal_periods,
            )
            res = model.fit(optimized=True)
            # statsmodels HWResults may not always exposer .aic; fallback to mse
            aic = getattr(res, 'aic', None)
            if aic is not None:
                return float(aic)
            # fallback: compute in-sample MSE
            fitted = res.fittedvalues
            mse = float(((self.df['montant'] - fitted) ** 2).mean())
            return mse
        except Exception as e:
            self._log(f"HoltWinters error: {e}")
            return float('inf')

    def _fit_lstm(self, look_back=12, epochs=10, batch_size=16):
        """
        Optionnel : petit modÃ¨le LSTM si `tensorflow` est installÃ©.
        Retourne une mÃ©trique de validation (MSE) ou inf si indisponible.
        """
        try:
            import numpy as _np
            from tensorflow.keras.models import Sequential
            from tensorflow.keras.layers import LSTM, Dense
            from tensorflow.keras.optimizers import Adam
        except Exception:
            self._log("TensorFlow non installÃ© â€“ LSTM ignorÃ©")
            return float('inf')

    def _fit_gru(self, look_back=12, epochs=10, batch_size=16):
        """
        Optionnel : modÃ¨le GRU (Gated Recurrent Unit) si TensorFlow disponible.
        Retourne une mÃ©trique de validation (MSE) ou inf si indisponible.
        """
        try:
            import numpy as _np
            from tensorflow.keras.models import Sequential
            from tensorflow.keras.layers import GRU, Dense
            from tensorflow.keras.optimizers import Adam
        except Exception:
            self._log("TensorFlow non installÃ© â€“ GRU ignorÃ©")
            return float('inf')

        try:
            series = self.df['montant'].astype('float32').values
            if len(series) < look_back * 2:
                self._log("Pas assez de donnÃ©es pour GRU")
                return float('inf')

            scaler = MinMaxScaler()
            series_s = scaler.fit_transform(series.reshape(-1, 1)).flatten()

            # PrÃ©parer windows
            X, y = [], []
            for i in range(len(series_s) - look_back):
                X.append(series_s[i:i + look_back])
                y.append(series_s[i + look_back])
            X = _np.array(X)
            y = _np.array(y)

            # split train/val
            split = int(len(X) * 0.8)
            X_train, X_val = X[:split], X[split:]
            y_train, y_val = y[:split], y[split:]

            X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
            X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], 1))

            model = Sequential([
                GRU(32, input_shape=(look_back, 1)),
                Dense(1)
            ])
            model.compile(optimizer=Adam(learning_rate=0.01), loss='mse')
            model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)
            val_pred = model.predict(X_val, verbose=0).flatten()
            mse = float(((y_val - val_pred) ** 2).mean())
            return mse
        except Exception as e:
            self._log(f"GRU training error: {e}")
            return float('inf')

    def _fit_rnn(self, look_back=12, epochs=10, batch_size=16):
        """
        Optionnel : modÃ¨le RNN vanilla (SimpleRNN) si TensorFlow disponible.
        Retourne une mÃ©trique de validation (MSE) ou inf si indisponible.
        """
        try:
            import numpy as _np
            from tensorflow.keras.models import Sequential
            from tensorflow.keras.layers import SimpleRNN, Dense
            from tensorflow.keras.optimizers import Adam
        except Exception:
            self._log("TensorFlow non installÃ© â€“ RNN ignorÃ©")
            return float('inf')

        try:
            series = self.df['montant'].astype('float32').values
            if len(series) < look_back * 2:
                self._log("Pas assez de donnÃ©es pour RNN")
                return float('inf')

            scaler = MinMaxScaler()
            series_s = scaler.fit_transform(series.reshape(-1, 1)).flatten()

            # PrÃ©parer windows
            X, y = [], []
            for i in range(len(series_s) - look_back):
                X.append(series_s[i:i + look_back])
                y.append(series_s[i + look_back])
            X = _np.array(X)
            y = _np.array(y)

            # split train/val
            split = int(len(X) * 0.8)
            X_train, X_val = X[:split], X[split:]
            y_train, y_val = y[:split], y[split:]

            X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
            X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], 1))

            model = Sequential([
                SimpleRNN(32, input_shape=(look_back, 1)),
                Dense(1)
            ])
            model.compile(optimizer=Adam(learning_rate=0.01), loss='mse')
            model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)
            val_pred = model.predict(X_val, verbose=0).flatten()
            mse = float(((y_val - val_pred) ** 2).mean())
            return mse
        except Exception as e:
            self._log(f"RNN training error: {e}")
            return float('inf')

    def _fit_sarimax_exog(self):
        """
        Optionnel : modÃ¨le SARIMAX avec variables exogÃ¨nes (trend).
        Retourne AIC ou inf si erreur.
        """
        try:
            # CrÃ©er une variable exogÃ¨ne (trend)
            trend = np.arange(len(self.df))
            
            model = SARIMAX(
                self.df['montant'],
                exog=trend.reshape(-1, 1),
                order=(1, 1, 1),
                seasonal_order=(1, 1, 1, 12),
                enforce_stationarity=False,
                enforce_invertibility=False
            )
            with warnings.catch_warnings():
                warnings.filterwarnings("ignore", category=ConvergenceWarning)
                results = model.fit(disp=False)
            self._log(f"SARIMAX exog (trend) fitted: AIC={results.aic:.2f}")
            return float(results.aic)
        except Exception as e:
            self._log(f"SARIMAX exog error: {e}")
            return float('inf')

    def _fit_var(self):
        """
        Optionnel : modÃ¨le VAR (Vector Autoregression).
        Utilise montant et une deuxiÃ¨me variable construite (ou tendance).
        Retourne AIC ou inf si indisponible/erreur.
        """
        try:
            from statsmodels.tsa.api import VAR
        except Exception:
            self._log("VAR non disponible (statsmodels version)")
            return float('inf')

        try:
            # CrÃ©er une variable auxiliaire (ex: moyenne mobile)
            aux_var = self.df['montant'].rolling(window=3, min_periods=1).mean()
            data = pd.DataFrame({'montant': self.df['montant'], 'trend': aux_var})
            data = data.dropna()
            
            if len(data) < 10:
                self._log("Pas assez de donnÃ©es pour VAR")
                return float('inf')
            
            model = VAR(data)
            results = model.fit(maxlags=1, ic='aic')
            self._log(f"VAR(1) fitted: AIC={results.aic:.2f}")
            return float(results.aic)
        except Exception as e:
            self._log(f"VAR error: {e}")
            return float('inf')

    def _fit_varma(self):
        """
        Optionnel : modÃ¨le VARMA (Vector ARMA).
        Retourne AIC ou inf si indisponible/erreur.
        """
        try:
            from statsmodels.tsa.statespace.varmax import VARMAX
        except Exception:
            self._log("VARMAX non disponible (statsmodels version)")
            return float('inf')

        try:
            # CrÃ©er variables
            aux_var = self.df['montant'].rolling(window=3, min_periods=1).mean()
            data = pd.DataFrame({'montant': self.df['montant'], 'trend': aux_var})
            data = data.dropna()
            
            if len(data) < 10:
                self._log("Pas assez de donnÃ©es pour VARMA")
                return float('inf')
            
            model = VARMAX(data, order=(1, 1))
            with warnings.catch_warnings():
                warnings.filterwarnings("ignore")
                results = model.fit(disp=False)
            self._log(f"VARMA(1,1) fitted: AIC={results.aic:.2f}")
            return float(results.aic)
        except Exception as e:
            self._log(f"VARMA error: {e}")
            return float('inf')

    def _fit_prophet(self):
        """
        EntraÃ®ne un modÃ¨le Prophet si disponible. Retourne MSE in-sample.
        """
        try:
            from prophet import Prophet
        except Exception:
            self._log("Prophet non installÃ© â€“ ignorÃ©")
            return float('inf')

        try:
            df_prop = self.df.reset_index().rename(columns={self.df.index.name or 'clean_date': 'ds', 'montant': 'y'})
            df_prop = df_prop[['ds', 'y']]
            m = Prophet()
            m.fit(df_prop)
            # in-sample prediction
            pred = m.predict(df_prop)
            y_true = df_prop['y'].values
            y_pred = pred['yhat'].values
            mse = float(((y_true - y_pred) ** 2).mean())
            return mse
        except Exception as e:
            self._log(f"Prophet error: {e}")
            return float('inf')

    def _forecast_prophet(self, steps):
        try:
            from prophet import Prophet
        except Exception:
            raise RuntimeError("Prophet non installÃ©")

        df_prop = self.df.reset_index().rename(columns={self.df.index.name or 'clean_date': 'ds', 'montant': 'y'})
        df_prop = df_prop[['ds', 'y']]
        m = Prophet()
        m.fit(df_prop)
        future = m.make_future_dataframe(periods=steps, freq='MS')
        forecast = m.predict(future)
        # take tail
        pred = forecast.tail(steps)
        dates = pd.to_datetime(pred['ds']).dt.strftime('%Y-%m-%d').tolist()
        values = pred['yhat'].tolist()
        upper = pred['yhat_upper'].tolist() if 'yhat_upper' in pred else values
        lower = pred['yhat_lower'].tolist() if 'yhat_lower' in pred else values
        return dates, values, upper, lower

    def _fit_cnn(self, look_back=12, epochs=10, batch_size=16):
        """Simple 1D-CNN for time series using TensorFlow/Keras if available.
        Returns validation MSE or inf if unavailable.
        """
        try:
            import numpy as _np
            from tensorflow.keras.models import Sequential
            from tensorflow.keras.layers import Conv1D, GlobalAveragePooling1D, Dense
            from tensorflow.keras.optimizers import Adam
        except Exception:
            self._log("TensorFlow non installÃ© â€“ CNN ignorÃ©")
            return float('inf')

        try:
            series = self.df['montant'].astype('float32').values
            if len(series) < look_back * 2:
                self._log("Pas assez de donnÃ©es pour CNN")
                return float('inf')

            scaler = MinMaxScaler()
            series_s = scaler.fit_transform(series.reshape(-1, 1)).flatten()

            X, y = [], []
            for i in range(len(series_s) - look_back):
                X.append(series_s[i:i + look_back])
                y.append(series_s[i + look_back])
            X = _np.array(X)
            y = _np.array(y)

            split = int(len(X) * 0.8)
            X_train, X_val = X[:split], X[split:]
            y_train, y_val = y[:split], y[split:]

            # Reshape to (samples, timesteps, features)
            X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
            X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], 1))

            model = Sequential([
                Conv1D(filters=16, kernel_size=3, activation='relu', padding='same', input_shape=(look_back, 1)),
                Conv1D(filters=8, kernel_size=3, activation='relu', padding='same'),
                GlobalAveragePooling1D(),
                Dense(1)
            ])

            model.compile(optimizer=Adam(learning_rate=0.01), loss='mse')
            model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)
            val_pred = model.predict(X_val, verbose=0).flatten()
            mse = float(((y_val - val_pred) ** 2).mean())
            return mse
        except Exception as e:
            self._log(f"CNN training error: {e}")
            return float('inf')

    def select_best_model(self):
        """Ã‰value plusieurs modÃ¨les (SARIMAX, HoltWinters, Prophet, LSTM, CNN)
        et choisit le meilleur selon un classement par rang (lower is better).
        Met Ã  jour `self.model_name`, `self.order`, `self.seasonal_order` si besoin.
        """
        self._log("Lancement de la sÃ©lection Ã©tendue de modÃ¨les (inclut HoltWinters/Prophet/DL si disponibles)")
        scores = {}

        # SARIMAX score (AIC)
        try:
            scores['SARIMAX'] = self._calculer_aic(self.order, self.seasonal_order)
        except Exception:
            scores['SARIMAX'] = float('inf')

        # SARIMAX avec variables exogÃ¨nes
        scores['SARIMAX_EXOG'] = self._fit_sarimax_exog()

        # VAR (Vector Autoregression)
        scores['VAR'] = self._fit_var()

        # VARMA (Vector ARMA)
        scores['VARMA'] = self._fit_varma()

        # Holt-Winters
        scores['HOLTWINTERS'] = self._fit_holtwinters(seasonal_periods=12)

        # Prophet
        scores['PROPHET'] = self._fit_prophet()

        # LSTM
        scores['LSTM'] = self._fit_lstm()

        # GRU
        scores['GRU'] = self._fit_gru()

        # RNN
        scores['RNN'] = self._fit_rnn()

        # CNN
        scores['CNN'] = self._fit_cnn()

        # Convert scores to ranks (1 = best)
        # lower score is better for all our metrics (AIC or MSE)
        ranked = sorted(scores.items(), key=lambda x: (float('inf') if x[1] is None else x[1]))
        ranks = {name: idx + 1 for idx, (name, _) in enumerate(ranked)}

        self._log(f"Scores modÃ¨les : {scores}")
        self._log(f"Ranks modÃ¨les : {ranks}")

        # Choose best by rank
        best = min(ranks.items(), key=lambda x: x[1])[0]
        self._log(f"Meilleur modÃ¨le selon sÃ©lection Ã©tendue : {best}")

        if best == 'HOLTWINTERS':
            self.model_name = 'HOLTWINTERS'
            self.order = (0, 0, 0)
            self.seasonal_order = (0, 0, 0, 0)
        elif best == 'PROPHET':
            self.model_name = 'PROPHET'
            self.order = (0, 0, 0)
            self.seasonal_order = (0, 0, 0, 0)
        elif best == 'LSTM':
            self.model_name = 'LSTM'
        elif best == 'CNN':
            self.model_name = 'CNN'
        else:
            # Keep SARIMAX/AR/ARMA/ARIMA selection
            self._log("Conserver la sÃ©lection SARIMAX/ARIMA classique")

        try:
            series = self.df['montant'].astype('float32').values
            if len(series) < look_back * 2:
                self._log("Pas assez de donnÃ©es pour LSTM")
                return float('inf')

            scaler = MinMaxScaler()
            series_s = scaler.fit_transform(series.reshape(-1, 1)).flatten()

            # PrÃ©parer windows
            X, y = [], []
            for i in range(len(series_s) - look_back):
                X.append(series_s[i:i + look_back])
                y.append(series_s[i + look_back])
            X = _np.array(X)
            y = _np.array(y)

            # split train/val
            split = int(len(X) * 0.8)
            X_train, X_val = X[:split], X[split:]
            y_train, y_val = y[:split], y[split:]

            X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
            X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], 1))

            model = Sequential([
                LSTM(32, input_shape=(look_back, 1)),
                Dense(1)
            ])
            model.compile(optimizer=Adam(learning_rate=0.01), loss='mse')
            model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)
            val_pred = model.predict(X_val, verbose=0).flatten()
            mse = float(((y_val - val_pred) ** 2).mean())
            return mse
        except Exception as e:
            self._log(f"LSTM training error: {e}")
            return float('inf')

    def calculate_and_validate_duration(self, user_months=None):
        """
        â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
        â”‚ SMART DURATION : DÃ©tecte la sparsity et valide la durÃ©e requise   â”‚
        â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        PROBLÃˆME RÃ‰SOLU :
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        Un utilisateur uploade un CSV avec donnÃ©es du 29-01-2020 et 29-01-2026.
        DiffÃ©rence : 6 ans = ~72 mois.
        
        Mais en rÃ©alitÃ©, il y a que 2 jours de donnÃ©es (29 janv 2020 + 29 janv 2026).
        Le fichier est TRÃˆS SPARSE (creux) !
        
        Si on utilise 72 mois pour SARIMA, les prÃ©visions seront:
          âŒ Peu fiables (prÃ©dire 12 mois de futur avec seulement 2 jours de donnÃ©es)
          âŒ Surapprendissage (overfitting)
          âŒ Intervalles de confiance Ã©normes (incertitude trÃ¨s haute)
        
        SOLUTION "INTELLIGENTE" :
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        Compter les mois RÃ‰ELS oÃ¹ le montant > 0 (data density).
        Diviser par 3 pour une durÃ©e sÃ»re (conservative approach).
        Appliquer des bornes (min 3, max 24 mois).
        
        ALGORITHME (4 Ã‰TAPES) :
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        
        ğŸ“Š Ã‰TAPE A : DÃ©tecter la sparsity
          â€¢ Compter les mois WHERE montant > 0
          â€¢ Calculer : densitÃ© = n_active / total_months
          â€¢ Si densitÃ© < 20%, alerter l'utilisateur
        
        ğŸ”¢ Ã‰TAPE B : Calculer durÃ©e sÃ»re
          â€¢ Formula : safe_duration = int(n_active / 3)
          â€¢ Ratio 1/3 = rÃ¨gle statistique (minimum 3 observations par paramÃ¨tre)
          â€¢ SARIMA(1,1,1)(1,1,1,12) = 8 paramÃ¨tres â†’ besoin d'au moins 24 obs
          â€¢ Donc : n_active=72 â†’ safe=24 mois âœ“
        
        ğŸ“ Ã‰TAPE C : Appliquer les bornes
          â€¢ Minimum : 3 mois (sinon pas assez de donnÃ©es)
          â€¢ Maximum : 24 mois (sinon prÃ©dictions trop loin = unreliable)
          â€¢ safe_duration = clamp(safe_duration, 3, 24)
        
        ğŸ¯ Ã‰TAPE D : DÃ©cider (selon user_months)
          â€¢ Si user_months = None (mode AUTO)
            â†’ Retourner safe_duration (le systÃ¨me dÃ©cide)
          
          â€¢ Si user_months > safe_duration
            â†’ Log un avertissement ğŸ”¶ avec explication
            â†’ Retourner safe_duration (sÃ©curitÃ© > demande utilisateur)
          
          â€¢ Si user_months <= safe_duration
            â†’ Log approuvÃ© âœ“
            â†’ Retourner user_months (faire confiance Ã  l'utilisateur)
        
        Args:
            user_months (int, optional): DurÃ©e demandÃ©e par l'utilisateur.
                Si None, mode AUTO (systÃ¨me dÃ©cide).
        
        Returns:
            int: Nombre de mois approuvÃ©s pour la prÃ©diction.
        
        Examples:
            df avec 24 mois actifs, user_months=None
              â†’ safe_duration = 24/3 = 8 â†’ clamped = 8
              â†’ return 8  âœ“
            
            df avec 72 mois actifs, user_months=36
              â†’ safe_duration = 72/3 = 24
              â†’ user_months (36) > safe_duration (24)
              â†’ Log avertissement ğŸ”¶
              â†’ return 24  (refuse la demande pour raison statistique)
            
            df avec 12 mois actifs, user_months=6
              â†’ safe_duration = 12/3 = 4 â†’ clamped = 4
              â†’ user_months (6) > safe_duration (4)
              â†’ Log avertissement ğŸ”¶
              â†’ return 4
        """
        self._log("\n" + "="*70)
        self._log("ğŸ“Š ANALYSE DE LA DENSITÃ‰ DES DONNÃ‰ES (Smart Duration)")
        self._log("="*70)
        
        try:
            # Ã‰TAPE A : DÃ©tection sparsity
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            total_months = len(self.df)
            active_months = (self.df['montant'] > 0).sum()  # Compter montant > 0
            data_density = (active_months / total_months) * 100 if total_months > 0 else 0
            
            self._log(f"ğŸ“ˆ PÃ©riode couverte : {total_months} mois")
            self._log(f"ğŸ“Š Mois ACTIFS (montant > 0) : {active_months}")
            self._log(f"ğŸ“‰ DensitÃ© : {data_density:.1f}%")
            
            if data_density < 20:
                self._log(f"âš ï¸  ATTENTION : DonnÃ©es TRÃˆS SPARSE (< 20%) - PrÃ©visions peu fiables")
                logger.warning(f"âš ï¸  Sparse data detected: {data_density:.1f}% active months")
            
            # Ã‰TAPE B : Calculer durÃ©e sÃ»re (rÃ¨gle 1/3)
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            safe_duration = int(active_months / 3)
            self._log(f"\nğŸ”¢ DurÃ©e brute (active_months / 3) : {active_months} / 3 = {safe_duration} mois")
            
            # Ã‰TAPE C : Appliquer bornes (min 3, max 24)
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            MIN_MONTHS = 3
            MAX_MONTHS = 24
            safe_duration = max(MIN_MONTHS, min(safe_duration, MAX_MONTHS))
            
            if safe_duration != int(active_months / 3):
                self._log(f"ğŸ“ AprÃ¨s clamping [3, 24] : {safe_duration} mois")
            
            # Ã‰TAPE D : DÃ©cision finale
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # Construire une raison lisible (sera utile pour l'API)
            reason_base = "MODE AUTO" if user_months is None else f"USER REQUEST ({user_months})"

            if user_months is None:
                # Mode AUTO
                final_reason = reason_base
                if data_density < 20:
                    final_reason += " - sparsity detected"
                self._log(f"\nâœ… MODE AUTO : DurÃ©e sÃ©lectionnÃ©e = {safe_duration} mois")
                self._log(f"ğŸ’¡ (Utilisateur n'a pas spÃ©cifiÃ©)")
                self._last_duration_reason = final_reason
                return safe_duration

            # --- MODE UTILISATEUR : Autoriser si raisonnable ---
            try:
                requested = int(user_months)
            except Exception:
                self._log("âš ï¸  Valeur months non numÃ©rique : rejetÃ©e")
                self._last_duration_reason = f"INVALID USER REQUEST ({user_months})"
                return safe_duration

            # Si la demande est infÃ©rieure ou Ã©gale Ã  la durÃ©e sÃ»re â†’ ok
            if requested <= safe_duration:
                self._log(f"\nâœ… APPROUVÃ‰ : {requested} mois (â‰¤ durÃ©e sÃ»re {safe_duration})")
                self._last_duration_reason = f"USER OVERRIDE ({requested} <= safe {safe_duration})"
                return requested

            # Si la demande est raisonnable (<= MAX_MONTHS) et ne dÃ©passe pas l'historique â†’ accepter
            MAX_MONTHS = 24
            total_months = total_months = len(self.df)
            if requested <= MAX_MONTHS and requested <= total_months:
                self._log(f"\nâœ… APPROUVÃ‰ (USER OVERRIDE) : {requested} mois (dans limites et historique suffisant)")
                self._last_duration_reason = f"USER OVERRIDE ({requested})"
                return requested

            # Sinon, rÃ©duire Ã  safe_duration
            self._log(f"\nâš ï¸  SÃ‰CURITÃ‰ STATISTIQUE âœ‚ï¸  DurÃ©e rÃ©duite")
            self._log(f"   â€¢ Demande : {requested} mois")
            self._log(f"   â€¢ Limite sÃ»re : {safe_duration} mois")
            self._log(f"   â€¢ Raison : DonnÃ©es insuffisantes pour prÃ©dire {requested} mois")
            self._log(f"   â€¢ DÃ©cision : Utiliser {safe_duration} mois (rejette {requested})")
            logger.info(f"âœ‚ï¸  Duration reduced: {requested} â†’ {safe_duration} (safety threshold)")
            self._last_duration_reason = f"USER REQUEST REDUCED ({requested} â†’ {safe_duration})"
            return safe_duration
                
        except Exception as e:
            self._log(f"\nâŒ Erreur lors de calculate_and_validate_duration : {str(e)}")
            self._log(f"âš ï¸  Utiliser durÃ©e par dÃ©faut : 12 mois")
            return 12

    def analyze_and_configure(self):
        """
        â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
        â”‚ SÃ‰LECTION AUTOMATIQUE & INTELLIGENTE DE MODÃˆLE                         â”‚
        â”‚ (Classique ARIMA/SARIMA + Deep Learning: HW, Prophet, LSTM, CNN)       â”‚
        â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        ALGORITHME :
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        1ï¸âƒ£  Diagnostique : ACF/PACF, stationnaritÃ© (ADF), saisonnalitÃ©
        2ï¸âƒ£  Ã‰valuer TOUS les modÃ¨les : SARIMA/ARIMA/AR/MA/ARMA + HW + Prophet + LSTM + CNN
        3ï¸âƒ£  Classer par mÃ©trique (AIC/MSE), choisir le MEILLEUR automatiquement
        
        Raises:
            Exception: Si erreur lors de l'analyse
        """
        self._log("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
        self._log("â•‘ SÃ‰LECTION AUTOMATIQUE & INTELLIGENTE DE MODÃˆLE                 â•‘")
        self._log("â•‘ (Classique + Deep Learning)                                    â•‘")
        self._log("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        
        try:
            # --- Ã‰TAPE 0 : Diagnostique initial ---
            self._log("\nğŸ“Š Ã‰TAPE 1 : DIAGNOSTIQUE DE LA SÃ‰RIE")
            self._log("â”€" * 60)
            
            # Test ADF (stationnaritÃ©)
            res_adf = adfuller(self.df['montant'].dropna())
            p_adf = res_adf[1]
            is_stationary = p_adf <= 0.05
            self._log(f"Test ADF: p-value = {p_adf:.4f} â†’ {'Stationnaire âœ“' if is_stationary else 'Non-stationnaire âœ—'}")
            
            # SaisonnalitÃ©
            has_seasonality = False
            season_amp = 0
            if len(self.df) >= 24:
                try:
                    decomp = seasonal_decompose(self.df['montant'], period=12)
                    season_amp = decomp.seasonal.max() - decomp.seasonal.min()
                    total_amp = self.df['montant'].max() - self.df['montant'].min()
                    has_seasonality = season_amp > 0.1 * total_amp
                    self._log(f"SaisonnalitÃ©: {'Oui âœ“' if has_seasonality else 'Non âœ—'} (amplitude={season_amp:.0f})")
                except Exception:
                    self._log("âš ï¸  Impossible de calculer saisonnalitÃ©")
            else:
                self._log("âš ï¸  Pas assez de donnÃ©es pour saisonnalitÃ© (< 24 mois)")
            
            # --- Ã‰TAPE 1 : Ã‰VALUER TOUS LES MODÃˆLES ---
            self._log("\nğŸ“ˆ Ã‰TAPE 2 : Ã‰VALUATION DE TOUS LES MODÃˆLES")
            self._log("â”€" * 60)
            
            model_scores = {}
            
            # 1a) ModÃ¨les ARIMA/SARIMA
            self._log("1ï¸âƒ£  ModÃ¨les ARIMA/SARIMA...")
            if has_seasonality and len(self.df) >= 24:
                aic_sarima = self._calculer_aic((1, 0, 1), seasonal_order=(1, 1, 1, 12))
                model_scores['SARIMA(1,0,1)(1,1,1,12)'] = aic_sarima
                self._log(f"   â€¢ SARIMA(1,0,1)(1,1,1,12): AIC={aic_sarima:.1f}")
            
            if not is_stationary:
                aic_arima = self._calculer_aic((1, 1, 1))
                model_scores['ARIMA(1,1,1)'] = aic_arima
                self._log(f"   â€¢ ARIMA(1,1,1): AIC={aic_arima:.1f}")
            else:
                # Tournoi AR/MA/ARMA
                aic_ar = self._calculer_aic((1, 0, 0))
                aic_ma = self._calculer_aic((0, 0, 1))
                aic_arma = self._calculer_aic((1, 0, 1))
                model_scores['AR(1)'] = aic_ar
                model_scores['MA(1)'] = aic_ma
                model_scores['ARMA(1,1)'] = aic_arma
                self._log(f"   â€¢ AR(1): AIC={aic_ar:.1f}")
                self._log(f"   â€¢ MA(1): AIC={aic_ma:.1f}")
                self._log(f"   â€¢ ARMA(1,1): AIC={aic_arma:.1f}")
            
            # 1b) Holt-Winters
            self._log("2ï¸âƒ£  Holt-Winters...")
            hw_score = self._fit_holtwinters()
            model_scores['HoltWinters'] = hw_score
            self._log(f"   â€¢ HoltWinters: AIC/MSE={hw_score:.1f}")
            
            # 1c) Prophet
            self._log("3ï¸âƒ£  Prophet...")
            prophet_score = self._fit_prophet()
            if prophet_score < float('inf'):
                model_scores['Prophet'] = prophet_score
                self._log(f"   â€¢ Prophet: MSE={prophet_score:.6f}")
            else:
                self._log(f"   â€¢ Prophet: non disponible")
            
            # 1d) Deep Learning (LSTM)
            self._log("4ï¸âƒ£  Deep Learning (LSTM)...")
            lstm_score = self._fit_lstm(look_back=12, epochs=10)
            if lstm_score < float('inf'):
                model_scores['LSTM'] = lstm_score
                self._log(f"   â€¢ LSTM: Validation MSE={lstm_score:.6f}")
            else:
                self._log(f"   â€¢ LSTM: non disponible")
            
            # 1e) Deep Learning (CNN)
            self._log("5ï¸âƒ£  Deep Learning (CNN)...")
            cnn_score = self._fit_cnn(look_back=12, epochs=10)
            if cnn_score < float('inf'):
                model_scores['CNN'] = cnn_score
                self._log(f"   â€¢ CNN: Validation MSE={cnn_score:.6f}")
            else:
                self._log(f"   â€¢ CNN: non disponible")
            
            # 1f) Deep Learning (GRU)
            self._log("6ï¸âƒ£  Deep Learning (GRU)...")
            gru_score = self._fit_gru(look_back=12, epochs=10)
            if gru_score < float('inf'):
                model_scores['GRU'] = gru_score
                self._log(f"   â€¢ GRU: Validation MSE={gru_score:.6f}")
            else:
                self._log(f"   â€¢ GRU: non disponible")
            
            # 1g) Deep Learning (RNN)
            self._log("7ï¸âƒ£  Deep Learning (RNN)...")
            rnn_score = self._fit_rnn(look_back=12, epochs=10)
            if rnn_score < float('inf'):
                model_scores['RNN'] = rnn_score
                self._log(f"   â€¢ RNN: Validation MSE={rnn_score:.6f}")
            else:
                self._log(f"   â€¢ RNN: non disponible")
            
            # 1h) SARIMAX avec exogÃ¨ne
            self._log("8ï¸âƒ£  SARIMAX (with exogenous trend)...")
            sarimax_exog_score = self._fit_sarimax_exog()
            if sarimax_exog_score < float('inf'):
                model_scores['SARIMAX_EXOG'] = sarimax_exog_score
                self._log(f"   â€¢ SARIMAX_EXOG: AIC={sarimax_exog_score:.1f}")
            else:
                self._log(f"   â€¢ SARIMAX_EXOG: non disponible")
            
            # 1i) VAR (Vector Autoregression)
            self._log("9ï¸âƒ£  VAR (Vector Autoregression)...")
            var_score = self._fit_var()
            if var_score < float('inf'):
                model_scores['VAR'] = var_score
                self._log(f"   â€¢ VAR: AIC={var_score:.1f}")
            else:
                self._log(f"   â€¢ VAR: non disponible")
            
            # 1j) VARMA (Vector ARMA)
            self._log("ğŸ”Ÿ VARMA (Vector ARMA)...")
            varma_score = self._fit_varma()
            if varma_score < float('inf'):
                model_scores['VARMA'] = varma_score
                self._log(f"   â€¢ VARMA: AIC={varma_score:.1f}")
            else:
                self._log(f"   â€¢ VARMA: non disponible")
            
            # --- Ã‰TAPE 2 : CLASSEMENT & CHOIX ---
            self._log("\nğŸ† Ã‰TAPE 3 : CLASSEMENT & CHOIX DU MEILLEUR MODÃˆLE")
            self._log("â”€" * 60)
            
            # Trier par score (ascending)
            sorted_models = sorted(model_scores.items(), key=lambda x: x[1] if x[1] < float('inf') else float('inf'))
            
            self._log("Classement (meilleur â†’ pire):")
            for idx, (name, score) in enumerate(sorted_models, 1):
                if score < float('inf'):
                    if score > 100:
                        self._log(f"   {idx}. {name}: {score:.1f}")
                    else:
                        self._log(f"   {idx}. {name}: {score:.6f}")
                else:
                    self._log(f"   {idx}. {name}: N/A (non disponible)")
            
            # Choix final
            best_model_name, best_score = sorted_models[0] if sorted_models else ("SARIMAX_DEFAULT", float('inf'))
            self._log(f"\nğŸ¯ MEILLEUR MODÃˆLE CHOISI : {best_model_name} (score={best_score:.1f})")
            
            # Set model_name, order, seasonal_order based on choice
            if 'SARIMA' in best_model_name:
                self.model_name = "SARIMA"
                self.order = (1, 0, 1)
                self.seasonal_order = (1, 1, 1, 12)
            elif 'ARIMA' in best_model_name:
                self.model_name = "ARIMA"
                self.order = (1, 1, 1)
                self.seasonal_order = (0, 0, 0, 0)
            elif 'AR(' in best_model_name:
                self.model_name = "AR"
                self.order = (1, 0, 0)
                self.seasonal_order = (0, 0, 0, 0)
            elif 'MA(' in best_model_name:
                self.model_name = "MA"
                self.order = (0, 0, 1)
                self.seasonal_order = (0, 0, 0, 0)
            elif 'ARMA' in best_model_name:
                self.model_name = "ARMA"
                self.order = (1, 0, 1)
                self.seasonal_order = (0, 0, 0, 0)
            elif 'HoltWinters' in best_model_name:
                self.model_name = "HoltWinters"
                self.order = (0, 0, 0)
                self.seasonal_order = (0, 0, 0, 0)
            elif 'Prophet' in best_model_name:
                self.model_name = "Prophet"
                self.order = (0, 0, 0)
                self.seasonal_order = (0, 0, 0, 0)
            elif 'LSTM' in best_model_name:
                self.model_name = "LSTM"
                self.order = (0, 0, 0)
                self.seasonal_order = (0, 0, 0, 0)
            elif 'CNN' in best_model_name:
                self.model_name = "CNN"
                self.order = (0, 0, 0)
                self.seasonal_order = (0, 0, 0, 0)
            
            self._log(f"\nâœ“ Configuration finale : model={self.model_name}, order={self.order}, seasonal={self.seasonal_order}")
            
        except Exception as e:
            self._log(f"âŒ ERREUR lors de l'analyse : {str(e)}")
            # Fallback
            self.model_name = "SARIMAX"
            self.order = (1, 0, 1)
            self.seasonal_order = (1, 1, 1, 12)
            raise

    def get_prediction_data(self, months=None):
        """
        â† CHANGEMENT 4 : EntraÃ®ne le modÃ¨le et retourne les prÃ©visions en DICTIONNAIRE.
        IntÃ¨gre Ã©galement la validation intelligente de la durÃ©e (Smart Duration).
        
        AVANT (autoPrediction.py) :
          plt.show()  âŒ Tente d'ouvrir une fenÃªtre graphique (impossible sur serveur)
        
        APRÃˆS (logic.py) :
          return {...}  âœ“ Retourne des donnÃ©es brutes (JSON-ready)
          Le FRONTEND (site web) utilisera ces donnÃ©es pour dessiner le graphique
        
        Args:
            months (int, optional): Nombre de mois Ã  prÃ©dire.
                - Si None : Mode AUTO (utilise calculate_and_validate_duration)
                - Si int : Mode UTILISATEUR (mais sera validÃ© par Smart Duration)
        
        Returns:
            dict: Dictionnaire JSON contenant :
            
            âœ“ SI SUCCÃˆS :
            {
                "status": "success",
                "model_info": {
                    "name": "SARIMA",
                    "order": "(1, 1, 1)",
                    "seasonal_order": "(1, 1, 1, 12)",
                    "aic": 150.5
                },
                "explanations": [
                    "SaisonnalitÃ© dÃ©tectÃ©e",                    venv\\Scripts\\Activate.ps1
                    streamlit run dashboard.py
                    "Choix : SARIMA",
                    ...
                ],
                "history": {
                    "dates": ["2020-01-01", "2020-02-01", ...],
                    "values": [1000.0, 1500.5, 1200.0, ...]
                },
                "forecast": {
                    "dates": ["2024-01-01", "2024-02-01", ...],
                    "values": [1400.0, 1450.0, ...],  # Valeurs prÃ©dites
                    "confidence_upper": [1500.0, 1550.0, ...],  # Intervalle sup. (95%)
                    "confidence_lower": [1300.0, 1350.0, ...]   # Intervalle inf. (95%)
                },
                "timestamp": "2025-12-25T15:30:45.123456",
                "duration_info": {
                    "requested_months": 12,
                    "validated_months": 12,
                    "reason": "MODE AUTO"
                }
            }
            
            âœ— SI ERREUR :
            {
                "status": "error",
                "error_message": "Description dÃ©taillÃ©e de l'erreur",
                "explanations": [...]
            }
        
        WORKFLOW :
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        1. Valider la durÃ©e (via calculate_and_validate_duration)
        2. EntraÃ®ner le modÃ¨le SARIMAX
        3. GÃ©nÃ©rer prÃ©visions + intervalles confiance
        4. Retourner dict JSON
        """
        # Valider et ajuster la durÃ©e (Smart Duration)
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        validated_months = self.calculate_and_validate_duration(user_months=months)
        
        # Utiliser la raison calculÃ©e par calculate_and_validate_duration si disponible
        reason = getattr(self, '_last_duration_reason', None)
        if not reason:
            reason = "MODE AUTO" if months is None else f"USER OVERRIDE ({months} â†’ {validated_months})"
        self._log(f"\nğŸ“Œ DurÃ©e FINALE pour prÃ©diction : {validated_months} mois ({reason})")
        
        self._log(f"\n=== GÃ‰NÃ‰RATION DE PRÃ‰VISIONS ({self.model_name}, {validated_months} mois) ===")
        
        try:
            # EntraÃ®ner le modÃ¨le final
            self._log(f"EntraÃ®nement SARIMAX | order={self.order} | seasonal={self.seasonal_order}")

            # FALLBACK : si la sÃ©rie est constante (variance nulle), Ã©viter SARIMAX et renvoyer une prÃ©vision naive
            if self.df['montant'].nunique() <= 1:
                last_value = float(self.df['montant'].iloc[-1])
                forecast_dates = [(self.df.index[-1] + pd.offsets.MonthBegin(i+1)).strftime('%Y-%m-%d') for i in range(validated_months)]
                return {
                    "status": "success",
                    "model_info": {
                        "name": "NAIVE_CONSTANT",
                        "order": str(self.order),
                        "seasonal_order": str(self.seasonal_order),
                        "aic": 0.0
                    },
                    "explanations": self.logs,
                    "history": {
                        "dates": [d.strftime('%Y-%m-%d') for d in self.df.index],
                        "values": self.df['montant'].tolist()
                    },
                    "forecast": {
                        "dates": forecast_dates,
                        "values": [last_value] * validated_months,
                        "confidence_upper": [last_value] * validated_months,
                        "confidence_lower": [last_value] * validated_months
                    },
                    "timestamp": datetime.now().isoformat(),
                    "duration_info": {
                        "requested_months": months,  # None si MODE AUTO
                        "validated_months": validated_months,
                        "reason": reason
                    }
                }

            model = SARIMAX(
                self.df['montant'],
                order=self.order,
                seasonal_order=self.seasonal_order,
                enforce_stationarity=False,
                enforce_invertibility=False
            )
            # Supprimer les ConvergenceWarning lors du fit (capturÃ©s et transformÃ©s en logs)
            with warnings.catch_warnings():
                warnings.filterwarnings("ignore", category=ConvergenceWarning)
                results = model.fit(disp=False)
            self._log(f"âœ“ ModÃ¨le entraÃ®nÃ© (AIC={results.aic:.2f})")
            
            # GÃ©nÃ©rer prÃ©visions avec intervalles de confiance
            forecast = results.get_forecast(steps=validated_months)
            pred = forecast.predicted_mean
            conf = forecast.conf_int()  # Intervalles 95% par dÃ©faut
            
            # â† KILLER FEATURE 1 : DÃ©tection d'anomalies (AI for Audit)
            # Utilise les rÃ©sidus du modÃ¨le pour dÃ©tecter les Ã©carts anormaux
            anomalies = self._detect_anomalies(results)
            
            # PrÃ©parer le dictionnaire retour (JSON-ready)
            return {
                "status": "success",
                "model_info": {
                    "name": self.model_name,
                    "order": str(self.order),
                    "seasonal_order": str(self.seasonal_order),
                    "aic": float(results.aic)
                },
                "explanations": self.logs,
                "history": {
                    "dates": [d.strftime('%Y-%m-%d') for d in self.df.index],
                    "values": self.df['montant'].tolist()
                },
                "forecast": {
                    "dates": [d.strftime('%Y-%m-%d') for d in pred.index],
                    "values": pred.tolist(),
                    "confidence_upper": conf.iloc[:, 1].tolist(),
                    "confidence_lower": conf.iloc[:, 0].tolist()
                },
                "anomalies": anomalies,
                "timestamp": datetime.now().isoformat(),
                "duration_info": {
                    "requested_months": months,  # None si MODE AUTO
                    "validated_months": validated_months,
                    "reason": reason
                }
            }
            
        except Exception as e:
            self._log(f"ERREUR lors de la prÃ©diction : {str(e)}")
            # MÃªme en cas d'erreur, retourner un dictionnaire (pas d'exception brute)
            return {
                "status": "error",
                "error_message": str(e),
                "explanations": self.logs
            }


    def _detect_anomalies(self, results):
        """
        â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
        â”‚ KILLER FEATURE 1 : DÃ©tection d'Anomalies (AI for Audit)                â”‚
        â”‚ Concept : Comparer l'historique rÃ©el avec ce qu'il AURAIT dÃ» Ãªtre      â”‚
        â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

        LOGIQUE :
        â”€â”€â”€â”€â”€â”€â”€â”€
        La TGR est un organisme de contrÃ´le. Leur plus grande peur : erreur/fraude.
        
        Au lieu de seulement prÃ©dire le FUTUR, on scanne le PASSÃ‰.
        
        Pour chaque mois historique :
          â€¢ Valeur rÃ©elle = montant enregistrÃ©
          â€¢ Valeur prÃ©dite = ce que le modÃ¨le aurait prÃ©dit (fitted values)
          â€¢ Ã‰cart (rÃ©sidu) = rÃ©el - prÃ©dit
        
        Si l'Ã©cart sort du "tunnel de sÃ©curitÃ©" (> 2Ïƒ ou 3Ïƒ), c'est SUSPECT.
        
        Exemple (donnÃ©es rÃ©elles TGR) :
          Janvier 2023 :
            DÃ©pense rÃ©elle : 5 millions DH
            DÃ©pense normale : 3 millions DH (selon modÃ¨le)
            Ã‰cart : 2 millions DH (2.5 Ã©carts-types)
            â†’ ANOMALIE DÃ‰TECTÃ‰E : "DÃ©pense 67% anormale"

        TECHNO :
        â”€â”€â”€â”€â”€â”€â”€
        Utilise les rÃ©sidus du modÃ¨le SARIMA (dÃ©jÃ  entraÃ®nÃ©).
        RÃ©sidus = erreurs du modÃ¨le = l'information "anormale".
        
        Seuils :
          â€¢ 1Ïƒ (68% confiance) : Normal dans variation
          â€¢ 2Ïƒ (95% confiance) : MOYEN (worth investigating)
          â€¢ 3Ïƒ (99.7% confiance) : Ã‰LEVÃ‰ (definite anomaly)

        Args:
            results: Objet results du SARIMAX entraÃ®nÃ©

        Returns:
            list: Liste d'anomalies dÃ©tectÃ©es
            
            Format d'une anomalie :
            {
                "date": "2023-03-01",
                "actual_value": 5000000.0,
                "predicted_value": 3000000.0,
                "residual": 2000000.0,
                "std_deviations": 2.5,
                "severity": "HIGH",
                "description": "DÃ©pense 67% supÃ©rieure Ã  la normale - Investigation recommandÃ©e"
            }
        """
        try:
            self._log("\n" + "=" * 70)
            self._log("ğŸ” DÃ‰TECTION D'ANOMALIES (AI for Audit)")
            self._log("=" * 70)

            anomalies = []

            # Obtenir les rÃ©sidus et les valeurs ajustÃ©es du modÃ¨le
            residuals = results.resid
            fitted_values = results.fittedvalues

            # Calculer l'Ã©cart-type des rÃ©sidus (mesure de variation "normale")
            std_residuals = residuals.std()
            mean_residuals = residuals.mean()

            self._log(f"ğŸ“Š Statistiques des rÃ©sidus :")
            self._log(f"   â€¢ Moyenne : {mean_residuals:.2f}")
            self._log(f"   â€¢ Ã‰cart-type : {std_residuals:.2f}")

            if std_residuals == 0:
                self._log("âš ï¸  Ã‰cart-type = 0. Pas d'anomalies dÃ©tectables.")
                return anomalies

            # DÃ©finir les seuils de sÃ©vÃ©ritÃ©
            # seuil_bas = 2Ïƒ (95% confiance)
            # seuil_haut = 3Ïƒ (99.7% confiance)
            threshold_medium = 2 * std_residuals
            threshold_high = 3 * std_residuals

            anomaly_count = 0

            # Parcourir tous les mois historiques
            for date, actual in self.df['montant'].items():
                # RÃ©cupÃ©rer la valeur prÃ©dite (fitted value)
                # Note : fitted_values a le mÃªme index que self.df
                if date in fitted_values.index:
                    predicted = fitted_values[date]
                    residual = actual - predicted

                    # Calculer l'Ã©cart en nombre d'Ã©carts-types
                    abs_residual_std = abs(residual) / std_residuals

                    # Classifier la sÃ©vÃ©ritÃ©
                    if abs_residual_std >= threshold_high / std_residuals:
                        severity = "HIGH"
                        emoji = "ğŸ”´"
                    elif abs_residual_std >= threshold_medium / std_residuals:
                        severity = "MEDIUM"
                        emoji = "ğŸŸ¡"
                    else:
                        severity = "LOW"
                        emoji = "ğŸŸ¢"

                    # Marquer comme anomalie si sÃ©vÃ©ritÃ© >= MEDIUM (> 2Ïƒ)
                    if abs_residual_std >= threshold_medium / std_residuals:
                        # Calculer un % de dÃ©viation lisible
                        pct_deviation = (abs(residual) / predicted * 100) if predicted != 0 else 0

                        description = (
                            f"{emoji} DÃ©pense {pct_deviation:.0f}% "
                            f"{'supÃ©rieure' if residual > 0 else 'infÃ©rieure'} Ã  la normale"
                        )

                        anomaly = {
                            "date": date.strftime('%Y-%m-%d'),
                            "actual_value": float(actual),
                            "predicted_value": float(predicted),
                            "residual": float(residual),
                            "std_deviations": float(abs_residual_std),
                            "severity": severity,
                            "description": description,
                        }
                        anomalies.append(anomaly)
                        anomaly_count += 1

                        self._log(f"  {emoji} {date.strftime('%B %Y')} : {description}")

            # Log rÃ©sumÃ©
            if anomaly_count == 0:
                self._log(f"\nâœ… Aucune anomalie dÃ©tectÃ©e (tous les rÃ©sidus < 2Ïƒ)")
            else:
                self._log(f"\nâš ï¸  {anomaly_count} anomalie(s) dÃ©tectÃ©e(s)")

            # Trier les anomalies par sÃ©vÃ©ritÃ© (HIGH en premier)
            severity_order = {"HIGH": 0, "MEDIUM": 1, "LOW": 2}
            anomalies.sort(key=lambda x: (severity_order.get(x["severity"], 3), x["std_deviations"]), reverse=True)

            return anomalies

        except Exception as e:
            self._log(f"âŒ Erreur lors de la dÃ©tection d'anomalies : {str(e)}")
            return []



def predict_from_file_content(file_content, months=None):
    """
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â”‚ FONCTION PRINCIPALE : Orchestre le pipeline complet                    â”‚
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    Cette fonction est le POINT D'ENTRÃ‰E de la logique de prÃ©diction.
    Elle coordonne les 3 Ã©tapes pour transformer du contenu binaire en JSON.
    
    â† CHANGEMENT 2 : ParamÃ¨tres function au lieu de input()
    â† CHANGEMENT : months est maintenant OPTIONNEL (None = MODE AUTO)
    
    AVANT (autoPrediction.py) :
      while True:
          file_path = input("Chemin du fichier ? ")
          months = input("Mois Ã  prÃ©dire ? ")
          predict(file_path, months)
      
      âŒ ProblÃ¨mes :
        â€¢ Bloque l'exÃ©cution (while True)
        â€¢ Demande l'input Ã  l'utilisateur (pas adaptÃ© Ã  une API)
        â€¢ Pas de gestion d'erreur structurÃ©e
    
    APRÃˆS (logic.py) :
      result = predict_from_file_content(file_content, months=None)
      
      âœ“ Avantages :
        â€¢ Non-bloquant : fonction retourne immÃ©diatement
        â€¢ ParamÃ¨tres viennent de la requÃªte HTTP (GET/POST)
        â€¢ Mode AUTO intelligent : durÃ©e calculÃ©e selon la densitÃ© des donnÃ©es
        â€¢ Gestion d'erreur centralisÃ©e
        â€¢ Retour structurÃ© (dictionnaire JSON)
    
    PIPELINE COMPLET :
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ file_content â”‚ --> â”‚  DataCleaner â”‚ --> â”‚SmartPredictorâ”‚ --> â”‚  Result  â”‚
    â”‚   (bytes)    â”‚     â”‚ (nettoyage)  â”‚     â”‚ (prÃ©diction) â”‚     â”‚  (JSON)  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         INPUT              Ã‰TAPE 1               Ã‰TAPE 2            OUTPUT
    
    Args:
        file_content (bytes): Contenu binaire du fichier CSV
            Exemple d'utilisation dans l'API :
            ```python
            from fastapi import UploadFile, Query
            from typing import Optional
            
            @app.post("/predict")
            async def predict(
                file: UploadFile,
                months: Optional[int] = Query(None, ge=1, le=60)
            ):
                file_bytes = await file.read()  # Lecture du fichier envoyÃ©
                result = predict_from_file_content(file_bytes, months=months)
                return result  # Retour JSON automatique
            ```
        
        months (int, optional): Nombre de mois Ã  prÃ©dire
            - Si None (dÃ©faut) : MODE AUTO
              â€¢ SystÃ¨me analyse densitÃ© des donnÃ©es
              â€¢ Calcule durÃ©e sÃ»re automatiquement
              â€¢ Utilisateur n'a pas Ã  se prÃ©occuper de la durÃ©e
            
            - Si int (ex: 12, 24) : MODE UTILISATEUR
              â€¢ Utilisateur demande une durÃ©e spÃ©cifique
              â€¢ SystÃ¨me valide via Smart Duration
              â€¢ Peut Ãªtre rÃ©duit si donnÃ©es insuffisantes
    
    Returns:
        dict: RÃ©sultat complet avec structure :
        
        âœ“ SI SUCCÃˆS :
        {
            "status": "success",
            "model_info": {...},
            "explanations": [...],
            "history": {...},
            "forecast": {...},
            "timestamp": "2025-12-25T15:30:45.123456",
            "duration_info": {
                "requested_months": null (ou int),
                "validated_months": 12,
                "reason": "MODE AUTO" ou "USER OVERRIDE"
            }
        }
        
        âœ— SI ERREUR :
        {
            "status": "error",
            "error_message": "Description dÃ©taillÃ©e de l'erreur",
            "explanations": [...]
        }
    
    GESTION DES ERREURS :
      â€¢ Try/except englobant tout le pipeline
      â€¢ Les erreurs Ã  chaque Ã©tape sont capturÃ©es
      â€¢ Un dictionnaire JSON est TOUJOURS retournÃ© (jamais d'exception brute)
      â€¢ Utile pour le frontend : il peut afficher l'erreur Ã  l'utilisateur
    
    Raises:
        Rien ! (toutes les exceptions sont capturÃ©es et retournÃ©es en JSON)
    """
    try:
        # Ã‰tape 1ï¸âƒ£  : NETTOYAGE ET PRÃ‰PARATION DES DONNÃ‰ES
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # RÃ´le : Transformer les bytes bruts en DataFrame propre (mensuel)
        # Sortie : DataFrame avec index=dates, colonne 'montant'=valeurs
        cleaner = DataCleaner(file_content)
        df_clean = cleaner.run()
        
        # Ã‰tape 2ï¸âƒ£  : ANALYSE ET SÃ‰LECTION DU MODÃˆLE
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # RÃ´le : Analyser la sÃ©rie et choisir le meilleur modÃ¨le
        # Sorties : model_name, order, seasonal_order + logs
        predictor = SmartPredictor(df_clean)
        predictor.analyze_and_configure()
        
        # Combiner les logs des deux Ã©tapes pour transparence maximale
        all_logs = cleaner.logs + predictor.logs
        
        # Ã‰tape 3ï¸âƒ£  : GÃ‰NÃ‰RATION DE PRÃ‰VISIONS
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # RÃ´le : EntraÃ®ner le modÃ¨le et gÃ©nÃ©rer les prÃ©visions
        # Sortie : Dictionnaire avec historique + prÃ©visions + intervalles
        result = predictor.get_prediction_data(months=months)
        
        # Ajouter tous les logs au rÃ©sultat final
        result["explanations"] = all_logs
        
        return result
        
    except Exception as e:
        # âŒ ERREUR : Retourner une structure JSON d'erreur (pas d'exception levÃ©e)
        return {
            "status": "error",
            "error_message": str(e),
            "explanations": []
        }
